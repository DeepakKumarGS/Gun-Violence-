---
title: "Violent Analysis"
author: "GSD"
date: "May 15, 2018"
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 10
    code_folding: hide
    fig_height: 4.5
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning = FALSE)
```

# Introduction 

The dataset is about the violence unleashed by gun's which are in the hands of people.This dataset provides an excellent platform to analyse various features and factors relating to gun violence and it can reveal hidden patterns once mined.

Lets get started 

# Loading the libraries & dataset 

We load the dataset and take a look at the dimensions.

```{r}
library(tidyverse)
library(janitor)
library(zoo)
library(data.table)
library(VIM)
library(gridExtra)
library(splitstackshape)
library(DescTools)
library(quanteda)
library(tm)
library(tidytext)
gun=fread("gun-violence-data_01-2013_03-2018.csv",header=TRUE,stringsAsFactors = FALSE)
dim(gun)
```

From the dimensions we understand that the dataset consist of 29 features or columns and has more than 2L line items.Let us inspect the null values in each colums and also the summary of the data.

```{r}
summary(gun)
```

The dataset variables are explained as follows:

* Incident Id - Unique ID to each incident
* date - Date of incident occurence
* State - State where incident happened
* City or Country - Country or city where incident took place
* Address - Location of crime
* n_killed - Number of people killed
* n_injured - Number of people injured
* incident_url - URL Describing the incident 
* source_url - Same as incident_url ('dataset description doesnt provide much info.Therefore we assume')
* incident_url_fields_missing- Logical indicating whether incident URL is present or not ('dataset description doesnt provide much info.Therefore we assume')
* congressional_district - District number (assume)
* gun_stolen- Status indicating whether the person had his/her gun stolen.
* gun_type - Type of gun
* incident_characteristics - Description of incident.
* latitute- Latitude of the area where crime happened.
* location_description - Place where incident took place.
* longitute - Longitude of the area where crime happened.
* n_guns_involved - Total guns involved in crime
* notes - Comments 
* participant_age - Age of people involved
* participant_age_group - Age bracket of the people involved
* participant_gender- Gender of involved people
* participant_name - Name of the involved people
* Participant_relationship - Relationship of the group
* participant_status - status of the people - either arrested,injured or killed in the incident
* participant_type - Either he is a victim or suspect
* sources - Source of the incident information
* state_house_district - state house district Number 
* state_senate_district - State senate district number

# Data Cleaning 

Before we dive into exploratory analysis,it is important that the data is cleaned to extract meaningful insights.Let us address the following problems one by one 
* Converting to factor variables & handling dates
* Removing useless informations
* Handling missing values 

## Converting to factor variables & handling dates

From the glimpse of the data we understand that there are some explicit variables that need to be treated as factors.Let us use the following code to covert them.

```{r}
factor_explicit =c("state","city_or_county")
gun[,(factor_explicit):=lapply(.SD,factor),.SDcols=factor_explicit]
sapply(gun,class)
```

For other columns,the data is provided such that both the factor levels and factor is in the same column seperated by || .We will use the `splitstackshape` package to deal with such columns.

But before that let us focus on the date column and convert them into proper date format from character.Lets first get the top 5 row items from date field.


```{r}
head(gun$date)
```

Perfect.Now using standard package,

```{r}
gun$date_formated=as.Date(gun$date,format="%Y-%m-%d")
head(gun$date_formated)

```

## Removing useless information 

We find that there are some columns which might be unnecessary for our exploratory data analysis.According to me,I would remove sources,incident_url,source_url,incident_url_fields_missing from the dataframe.Lets remove it,

```{r}
remove <- c("sources","source_url","incident_url_fields_missing","incident_url")
gun[,(remove):=NULL]
names(gun)
```

Perfect.Lets move on,

## Handling Missing Values :

Let us see now how many columns have missing values in them and their count.

```{r}
na_values <-gun %>% map_dbl(~sum(is.na(.)))
round((na_values/nrow(gun))*100,2)
```

From the  proportion of missing values summary it is understood that the dataset is having heavy missing values.Lets visualise them to understand better.

```{r}
aggr(gun, prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)
```

The table gives a summary of the missing values sorted in descending order whereas the visual represents the combination of missing values in each column.

We also find that except for n_guns_involved column which has 41 % of missing values all the other proportions of missing values in the column is lower.

Lets start our most interesting part ...

# Exploratory Data Analysis:

```{r}
theme_function=function() {theme_bw()+theme(legend.position="none",plot.title=element_text(size=16,hjust=0.5),axis.text.x=element_text(angle=90))}

```

## Distribution of Incidents over state:

We want to get a birdseye view of the number of incidents that have taken place over the state.Lets visualise them.

```{r,fig.width=13}
ggplot(gun,aes(state,..count..))+geom_histogram(stat="count",fill="blue")+theme_function()+labs(x="State",y="Count",title="")

```

We find that the states -  Illinos,California,Texas have higher incidents of gun shooting.

## Trend of incidents over time.

Lets understand whether there were any patterns in the incidents over the year,month.

```{r}
gun$year=as.factor((year(gun$date)))
gun$month=lubridate::month(gun$date,label=TRUE,abbr=TRUE)
#temp=gun %>% group_by(year) %>% summarise(count=n())
temp=gun %>% group_by(year,month) %>% summarise(count=n())
ggplot(temp,aes(month,count,color=year))+geom_line(aes(group=year),size=0.9)+geom_point(alpha=0.5)+theme_bw()+theme(legend.position="bottom")+labs(x="Month",y="Count of incidents")
```

We find that the year 2013 has recorded least incidents ( atleast according to the data or we can assume that most of the incidents were not recorded).After 2013 the number of incidents were never <2000 in any month.Therefore our assumption can be safely considered.We see a overall trend in the month of feb ,June,July where there is a decline from the immediate previous month.

The data is consistent for the years 2015 to 2016.For 2018 we have only 3 month data.Is there any daily trend ? Lets find out.

```{r}
gun$day=lubridate::wday(gun$date,label=TRUE,abbr=TRUE)

temp=gun %>% group_by(year,day) %>% summarise(count=n())
ggplot(temp,aes(day,count,color=year))+geom_line(aes(group=year),size=0.9)+geom_point(alpha=0.5)+theme_bw()+theme(legend.position="bottom",axis.text.x = element_text(angle=90))+labs(x="Day",y="Count of incidents")
```

We find that for the years 2018 and 2013 ,we cant conclude anything since the data is incomplete.

There is a general trend observed over the years as seen from the graph.Sundays has a rise in the incidents whereas Mon and Tue are comparatively lower.

## Time Series for Number of people killed and injured:

Let us now visualise the number of people killed or injured in the incident as a function of time.

```{r,fig.height=7,fig.width=6}
temp = gun %>% group_by(year) %>% summarise(people_killed=sum(n_killed),people_injured=sum(n_injured),number_of_incidents=n())
g1=ggplot(temp,aes(number_of_incidents,people_killed))+geom_point(aes(size=people_killed,col=year))+theme_function()+theme(legend.position="bottom")+labs(x="Number of incidents",y="People Killed",title="",size="People Killed",col="Year")
g2=ggplot(temp,aes(number_of_incidents,people_injured))+geom_point(aes(size=people_killed,col=year))+theme_function()+theme(legend.position="bottom")+labs(x="Number of incidents",y="People Injured",title="",size="People Injured",col="Year")
grid.arrange(g1,g2,nrow=2)
```

We find that there is a perfect cause and effect relationship.As the number of incidents rises ,the number of people injured or killed has rised for the year.This is nothing strange and is very normal.

## Age distribution of preparators

We want to find out how the preparators age is distributed.

Now lets have a sneek peak of the column where the age details are present.

```{r}
head(gun$participant_age)
```

Hmm,inorder to have a histogram we require some cleaning in this colum.This is where `splitstackshape` comes handy.Refer the code below where i do the cleaning part.This package presents some advantages in these situations where we need to split a row having multiple values into rows having only one value each.What makes it different is that the functions are capable of handling rows that have multiple values whose count is different. Have a look at my other [kernel](https://www.kaggle.com/gsdeepakkumar/startup-funding) where I have applied this package for analysis.


```{r}
age=cSplit(gun,c("participant_age"),sep="||",direction="long",drop=FALSE)
age$age_split=gsub(".*::","",age$participant_age)
age$age=as.numeric(age$age)
head(age$age)
```

Perfect.Now lets do the visualisation and infer the findings.

```{r}
ggplot(age,aes(x="Age",age))+geom_boxplot(fill="blue")+theme_function()+labs(x="",y="Age")
```

We see a large number of outliers and there are also wierd values like 300,220 !!!!!.

## Gun Type 

Now let us use the same method to know about the type of gun used.

```{r}
gun_type=cSplit(gun,c("gun_type"),sep="||",direction="long",drop=FALSE)
gun_type$type_split=gsub(".*:","",gun_type$gun_type)
temp= gun_type %>% group_by(type_split) %>% summarise(count=n())%>% arrange(desc(count))
ggplot(temp,aes(factor(type_split,type_split),count,fill=type_split))+geom_bar(stat="identity")+theme_function()+labs(x="Gun Type",y="Count",title="")
```


We see that there are many instances where the gun type was unknown.Handgun,9mm pistol and Rifle were used by preparators.

## Incident Characteristics:

Lets visualise the characteristics of each incident.

```{r}
incident_chara=cSplit(gun,c("incident_characteristics"),sep="||",direction="long",drop=FALSE)
temp= incident_chara %>% group_by(incident_characteristics) %>% summarise(count=n())%>% arrange(desc(count))
ggplot(head(temp,20),aes(factor(incident_characteristics,incident_characteristics),count,fill=incident_characteristics))+geom_bar(stat="identity")+theme_function()+labs(x="Incident Characteristics",y="Count",title="")+coord_flip()
```

Since the output gave 600+ types,I visualized the top 20 characteristics.We see that most of the time the person is injured or died once shot.And there were also many cases where the incident was non-shooting or persons escaped with no injuries.

## Participant status:

It is important to know what happened after the incident happened.The dataset provides information about the type of participant(either victim or suspect) and what the person's status was after the incident.Lets use these features to visualise and understand the scenario.

Since splitting both the columns runs to several millions,I restrict the analysis to univariate and to the year that has seen highest incidents. = 2017

```{r,fig.width=10}
gun_filter= gun %>% filter(year=='2017')
type=cSplit(gun_filter,c("participant_status"),sep="||",direction="long",drop=TRUE)
type$participant_status=gsub(".*:","",type$participant_status)
head(type$participant_status)
total=length(unique(type$incident_id))
temp = type %>% group_by(participant_status) %>% summarise(count=n()) %>% mutate(perc=round((count/total)*100,2)) %>% arrange(desc(count))
ggplot(temp,aes(factor(participant_status,participant_status),count,fill=participant_status))+geom_bar(stat="identity")+theme_function()+geom_text(aes(label=paste0(perc,"%",sep=" ")),hjust=0,vjust=0.5,size=4,color='black',fontface='bold')+labs(x="Participant Status",y="Count",title='Year:2017 - Participant Status')+coord_flip()
```

As infered from previous graphs,we find that nearly in half of the instances people were injured followed closely by arrest.28 % of the people were killed where as nearly same amount were unharmed.

# Text Analytics

Next with the notes column we do some text analysis.Here I will be extensively using functions from package `quanteda` which i stumbled upon a few months back !!! .It has some very good functions for text analytics.

For the purpose of analysis,let us extract notes column into a seperate df.

```{r}
text=gun$notes
head(text)
```

Now for creating a wordcloud,lets create a corpus from the text.

```{r}
corp=corpus(text)
head(summary(corp))
```

## Analysing some keywords:

`quanteda` provides a function called `kwic` which expands to keyword in context.Lets analyse the text with keywords - `killed`,`police`,`terrorist` and see if there are any hits.We pass this to the function as a vector and extract the output.Pls refer the code for more details.I have given a regex type of match which means that even a partial match of the keyword will be thrown out.

```{r}
kw <- c('killed','police','terrorist')
head(kwic(corp,phrase(kw),window=2,valuetype="regex"),10)
```

Thus we find that the mentioned keywords find a place in the notes .

## Wordcloud:

Now ,lets visualise the text using the wordcloud.

```{r,fig.width=8,fig.height=6}
wc <- corp %>% dfm(remove=stopwords('en'),remove_punct=TRUE)
set.seed(100)
textplot_wordcloud(wc,random_color = TRUE,max_count=10,random_order=TRUE,rotation=0.2,max_words = 50,color="darkred",max_size = 8,min_size = 0.8)
```

## Topic Modelling 

Now that we are done with the keyword search lets play with the text and find out whether the words echo more than one theme.

```{r}
topic <- dfm(corp,verbose = FALSE,remove_punct=TRUE,remove=stopwords('en'))
dtm <- convert(topic,to="topicmodels")
lda <- topicmodels::LDA(dtm,k=2,control=list(seed=100))
gun_topics <- tidy(lda,matrix="beta")

```

Now that we have applied LDA lets visualise,

```{r}
topic_top_terms <- gun_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

topic_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


From the visuals,we understand that the top 3 words in the 2 groups have been more or less the same meaning that all the incidents have one characteristic in particular that can be expressed in only one way.

# Conclusion 

Thus with this dataset we analysed various factors that were tied to the gun violence and made text analysis to mine some valuable insights.The dataset presented an unique challenge where there were few columns having row values that were a mini row values in themselves !!!! . We used standard available libraries in r to tackle this problem and successfully 'tidied' the data for analysis pupose.
